{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f28ebfe",
   "metadata": {},
   "source": [
    "# Implement the forward propagation for a two hidden layer network for m-samples, n-features as we discussed in class. Initialize the weights randomly. Use the data from the previous labs like logistic regression. You can choose the number of neurons in the hidden layer and use sigmoid activation function.Report the evaluation metrics for the network.  Also use other non-linear activation functions like ReLU and Tanh. Report the loss using both MSE and Cross Entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dabe262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.395</td>\n",
       "      <td>7.638</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.987</td>\n",
       "      <td>6.485</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.358</td>\n",
       "      <td>6.499</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.036</td>\n",
       "      <td>2.380</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.956</td>\n",
       "      <td>7.378</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.304</td>\n",
       "      <td>1.608</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>6.140</td>\n",
       "      <td>4.261</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>6.579</td>\n",
       "      <td>6.231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2.555</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>2.148</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        x1     x2  label\n",
       "0    7.395  7.638      1\n",
       "1    4.987  6.485      1\n",
       "2    5.358  6.499      1\n",
       "3    2.036  2.380      0\n",
       "4    5.956  7.378      1\n",
       "..     ...    ...    ...\n",
       "495  0.304  1.608      0\n",
       "496  6.140  4.261      1\n",
       "497  6.579  6.231      1\n",
       "498  2.555  0.446      0\n",
       "499  2.148  0.852      0\n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Logistic_regression_ls.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "76e0dd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 500)\n",
      "(500, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = df[['x1','x2']].values\n",
    "print(num_samples.T.shape)\n",
    "y = df[['label']].values\n",
    "print(y.shape)\n",
    "num_samples.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc14d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "12e8b633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared loss for activation sigmoid: 0.4262\n",
      "Mean squared loss for activation tanh: 0.4268\n",
      "Mean squared loss for activation relu: 0.5000\n",
      "cross entropy loss for activation sigmoid: 1.3034\n",
      "cross entropy loss for activation tanh: 1.3075\n",
      "cross entropy loss for activation relu: 5.7191\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def activation(x,activ):\n",
    "    if activ == 'sigmoid':\n",
    "        return 1/(1+np.exp(-x)) \n",
    "    if activ == 'relu':\n",
    "        return np.maximum(0, x)\n",
    "    if activ == 'tanh':\n",
    "        return np.tanh(x) \n",
    "    \n",
    "def mean_squard_loss(y_true,y_pred):\n",
    "    loss = np.mean((y_true - y_pred)**2)\n",
    "    return loss\n",
    "    \n",
    "def cross_entropy_loss(y_true,y_pred):\n",
    "    eps = 1e-5\n",
    "    y_pred = np.clip(y_pred,eps,1-eps)\n",
    "    yp_log1 = np.log(y_pred)\n",
    "    yp_log2 = np.log(1-y_pred)\n",
    "    loss = -np.mean(y_true*yp_log1) - np.mean((1 - y_true)*yp_log2)\n",
    "    return loss\n",
    "\n",
    "# Input_size is the number of features \n",
    "input_size = num_samples.shape[1]\n",
    "num_hidden_layers = 2\n",
    "nodes_in_hidden_layers = [10,4]\n",
    "output_size = 1\n",
    "\n",
    "w1 = np.random.rand(input_size, nodes_in_hidden_layers[0])  # shape of w1 (2,10)\n",
    "b1 = np.random.rand(nodes_in_hidden_layers[0],1) # shape of b1 (10,1)\n",
    "\n",
    "w2 = np.random.rand(nodes_in_hidden_layers[0],nodes_in_hidden_layers[1])  # shape of w2 (10,4)\n",
    "b2 = np.random.rand(nodes_in_hidden_layers[1],1)  # shape of w1 (4,1)\n",
    "\n",
    "w3 = np.random.rand(nodes_in_hidden_layers[1],output_size) # shape of w3 (1,1)\n",
    "b3 = np.random.rand(output_size,1)   # shape of w1 (1,1)\n",
    "\n",
    "def forward_prop(x,w1,w2,w3,b1,b2,b3, activ):\n",
    "                                # shape of x (2,500) and number of nodes in 1st and 2nd hidden layers are 10 and 4\n",
    "    a1 = np.dot(w1.T,x) + b1    # shape of a1 (10,500)\n",
    "    h1 = activation(a1,activ)   # shape of h1 (10,500)\n",
    "    \n",
    "    a2 = np.dot(w2.T,h1) + b2   # shape of a2 (4,500)\n",
    "    h2 = activation(a2,activ)   # shape of h2 (4,500)\n",
    "    \n",
    "    a3 = np.dot(w3.T, h2) + b3  # shape of a3 (1,500)\n",
    "    y_pred = sigmoid(a3)  # shape of a1 (1,500)\n",
    "    \n",
    "    return y_pred.T      # output shape (500,1)\n",
    "\n",
    "x = num_samples.T  # shape of x (2,500)\n",
    "y_true = y     # shape of y_true (500,1)\n",
    "y_pred1 = forward_prop(x,w1,w2,w3,b1,b2,b3, 'sigmoid')   # shape of y_pred (500,1)\n",
    "y_pred2 = forward_prop(x,w1,w2,w3,b1,b2,b3, 'tanh')\n",
    "y_pred3 = forward_prop(x,w1,w2,w3,b1,b2,b3, 'relu')\n",
    "\n",
    "print(f\"Mean squared loss for activation sigmoid: {mean_squard_loss(y_true,y_pred1):.4f}\" )\n",
    "print(f\"Mean squared loss for activation tanh: {mean_squard_loss(y_true,y_pred2):.4f}\" )\n",
    "print(f\"Mean squared loss for activation relu: {mean_squard_loss(y_true,y_pred3):.4f}\" )\n",
    "\n",
    "print(f\"cross entropy loss for activation sigmoid: {cross_entropy_loss(y_true,y_pred1):.4f}\" )\n",
    "print(f\"cross entropy loss for activation tanh: {cross_entropy_loss(y_true,y_pred2):.4f}\" )\n",
    "print(f\"cross entropy loss for activation relu: {cross_entropy_loss(y_true,y_pred3):.4f}\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3baa6b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    # Calculate True Positives, False Positives, True Negatives, False Negatives\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    # Calculate Precision, Recall, F1-Score, Accuracy\n",
    "    P = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "    R = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    f1 = 2 * (P * R) / (P + R) if (P + R) != 0 else 0\n",
    "    acc = (TP + TN) / len(y_true)\n",
    "    return P, R, f1, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e3420c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the output probabilities to labels 0 or 1 for probability greater than 0.5\n",
    "y_pred1_labels = np.where(y_pred1 > 0.5, 1, 0)\n",
    "y_pred2_labels = np.where(y_pred2 > 0.5, 1, 0)\n",
    "y_pred3_labels = np.where(y_pred3 > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4896e20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation matrix for activation sigmoid\n",
      "Precision:  0.5\n",
      "Recall:  1.0\n",
      "F1_score:  0.6666666666666666\n",
      "Accuracy: 50.0%\n",
      "\n",
      "Evaluation matrix for activation Tanh\n",
      "Precision:  0.5\n",
      "Recall:  1.0\n",
      "F1_score:  0.6666666666666666\n",
      "Accuracy: 50.0%\n",
      "\n",
      "Evaluation matrix for activation relu\n",
      "Precision:  0.5\n",
      "Recall:  1.0\n",
      "F1_score:  0.6666666666666666\n",
      "Accuracy: 50.0%\n"
     ]
    }
   ],
   "source": [
    "precision1, recall1, f1_score1, accuracy1 = evaluate_metrics(y_true, y_pred1_labels)\n",
    "print(\"Evaluation matrix for activation sigmoid\")\n",
    "print(\"Precision: \",precision1)\n",
    "print(\"Recall: \",recall1)\n",
    "print(\"F1_score: \",f1_score1)\n",
    "print(f\"Accuracy: {precision1*100}%\")\n",
    "print()\n",
    "precision2, recall2, f1_score2, accuracy2 = evaluate_metrics(y_true, y_pred2_labels)\n",
    "print(\"Evaluation matrix for activation Tanh\")\n",
    "print(\"Precision: \",precision2)\n",
    "print(\"Recall: \",recall2)\n",
    "print(\"F1_score: \",f1_score2)\n",
    "print(f\"Accuracy: {precision2*100}%\")\n",
    "print()\n",
    "precision3, recall3, f1_score3, accuracy3 = evaluate_metrics(y_true, y_pred3_labels)\n",
    "print(\"Evaluation matrix for activation relu\")\n",
    "print(\"Precision: \",precision3)\n",
    "print(\"Recall: \",recall3)\n",
    "print(\"F1_score: \",f1_score3)\n",
    "print(f\"Accuracy: {precision3*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a1fcc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
